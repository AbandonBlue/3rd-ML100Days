{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習時間"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請觀看李宏毅教授以神奇寶貝進化 CP 值預測的範例，解說何謂機器學習與過擬合。並回答以下問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[youtube](https://www.youtube.com/watch?v=fegAeph9UaA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 模型的泛化能力 (generalization) 是指什麼？ \n",
    "\n",
    "- 就是**模型訓練完**之後，利用**實際資料**實際跑模型，預測結果的好壞。\n",
    "    通常在前處理時，就會將資料切分，留下一些資料測試，\n",
    "    就千萬不能用訓練資料去預測，因為模型參數就是透過訓練資料訓練出來的，所以當然表現結果必然是好的。\n",
    "\n",
    "\n",
    "\n",
    "### 2. 分類問題與回歸問題分別可用的目標函數有哪些？\n",
    "\n",
    "- 分類問題\n",
    "    - 交叉熵(cross-entropy)\n",
    "    - 0-1損失函數\n",
    "    - 感知損失函數（Perceptron Loss）\n",
    "    - Hinge損失函數（hinge loss function）\n",
    "    - 對數損失函數（Log Loss）\n",
    "\n",
    "原文網址：https://kknews.cc/code/9opor8j.html\n",
    "\n",
    "- 回歸問題\n",
    "    - MSE, 均方誤差, 也稱L2\n",
    "    - MAE, 平均絕對誤差, 也稱L1\n",
    "    - Huber 損失, 平滑的平均絕對誤差, 上述兩者的綜合版本\n",
    "    - Log-Cosh 損失\n",
    "    - 理解分位數損失函數\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
